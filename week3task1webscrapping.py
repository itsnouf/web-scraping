# -*- coding: utf-8 -*-
"""Week3Task1WebScrapping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e7N_XFOGG3kWLCxW0rTqDP3rGUz2UHUr

# **1. Perform Web Scraping to extract the information from the website.**
https://scrapeme.live/shop/
"""

# Imports
import requests
import pandas as pd
from bs4 import BeautifulSoup

pokemon="https://scrapeme.live/shop/"

#Â to download the html page
pokemon_response=requests.get(pokemon)
pokemon_response #200 ðŸ‘ŒðŸ‘ŒðŸ‘ŒðŸ‘ŒðŸ‘Œ

from google.colab import drive
drive.mount('/content/drive')

pokemonSoup=BeautifulSoup(pokemon_response.text,'html.parser') #parse the html
pokemonSoup

pokemonSoup.title

pokemonCatagory=pokemonSoup.find_all('a',{'class':'woocommerce-LoopProduct-link woocommerce-loop-product__link'})
pokemonCatagory

pokemon_urls = [pokemon['href'] for pokemon in pokemonCatagory]
print(pokemon_urls)
base_url = 'https://scrapeme.live/shop/Ivysaur/'
full_urls = [base_url + href for href in pokemon_urls]
print(full_urls)

len(full_urls)

#each product is stored in its own div page so using for loop to store them all in one list

pokemonList=[] #to store urls of prodcuts

for i in pokemonSoup.find_all("a", class_="woocommerce-LoopProduct-link"):
    pokemonList.append(i["href"])
pokemonList

"""for starter i am going to save the bame and price"""

#storing product dets from each page by revisiting each link

def get_product_details(url):
    response = requests.get(url, verify=False)

    soup = BeautifulSoup(response.text, "html.parser")

    name_tag = soup.find("h1", class_="product_title entry-title")
    product_name = name_tag.text.strip() if name_tag else None

    price_tag = soup.find("span", class_="price")
    product_price = price_tag.text.strip() if price_tag else None

    desc_tag = soup.find("div", class_="woocommerce-product-details__short-description")

    description = desc_tag.get_text(strip=True) if desc_tag else None
    Produt_tags = [a.get_text(strip=True) for a in soup.select("span.tagged_as a")]
    stock_tag = soup.find("p", class_="stock")
    stockStatues = stock_tag.get_text(strip=True) if stock_tag else None
    SKU_tag=soup.find("span",class_="sku")
    SKU=SKU_tag.get_text(strip=True) if SKU_tag else None
    product_catagory=soup.find("span",class_="posted_in").text.strip()

    product_details={
        "product_name":product_name,
        "product_price":product_price,
        "Description": description,
        "Tags": Produt_tags,
        "stock_status":stockStatues,
        "SKU":SKU,
        "product_catagory":product_catagory
    }

    return product_details

pokemon_products=[]
page_num=[]
page_url = "https://scrapeme.live/shop/"

while True:
  response = requests.get(page_url, verify=False)


  soup = BeautifulSoup(response.text, "html.parser")

  #current page link
  product_link = [a['href'] for a in soup.find_all("a", class_="woocommerce-LoopProduct-link")]
  #loop stops when no product there
  if not product_link:
    break

  for product in product_link:
    product_data=get_product_details(product)
    pokemon_products.append(product_data)

  next_page = soup.select_one("nav.woocommerce-pagination a.next.page-numbers")

  if not next_page:
    print("No more pages")
    break
  page_url = next_page['href']

#pokemon_products
df=pd.DataFrame(pokemon_products)
df.to_csv("pokemon.csv",index=False)
print("Scraping complete and the data is saved on pokemon.csv file")

df